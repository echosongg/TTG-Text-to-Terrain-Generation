{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca36ee2e-ec4f-4c30-9a4f-8c757d02a288",
   "metadata": {},
   "source": [
    "# Step 2: The text description generates the terrain distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dcab3a-6c1d-4530-8120-ef37e1b6c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089768d8-37ec-4257-acde-81d16bf85aa5",
   "metadata": {},
   "source": [
    "## 1.0 Hyperparameters and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221c9fa8-fc57-41c9-a880-2ffde45cd5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUM_DISTRIBUTION = 256\n",
    "INTER_DIM =31\n",
    "HIDDEN_DIM_1 = [64, 32, 16]\n",
    "HIDDEN_DIM_2 = [128, 64, 32, 16, 32]\n",
    "LABEL_NUM = 5\n",
    "BATCH_SIZE_2 = 4\n",
    "EPOCH_2 = 50\n",
    "LEARNING_RATE_2 = 0.0005\n",
    "TRAIN_PATH_2 = \"../data/text/prompt_train.csv\"\n",
    "VALID_PATH_2 = \"../data/text/prompt_valid.csv\"\n",
    "TEST_PATH_2 = \"../data/text/prompt_test.csv\"\n",
    "SAVE_MODEL_PATH_1 = \"../model/best_classifier.pth\"\n",
    "SAVE_MODEL_PATH_2 = \"../model/best_generator.pth\"\n",
    "OUTPUT_PATH = \"../test_distribution.csv\"\n",
    "PREDICTION_PATH = \"../test_prediction.csv\"\n",
    "TRUE_LABEL_PATH = \"../test_true_label.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ebebe-1d20-4d97-b20f-c9983b9fc301",
   "metadata": {},
   "source": [
    "## 2.1 Load Data Model and Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1740a778-10df-48c0-addd-1cd6ccf1b4d4",
   "metadata": {},
   "source": [
    "### 2.1.1 Word2Vec (Only one of 2.1.1 and 2.1.2 can be chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83502d25-d330-4d31-a0f8-b0027617576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaders created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Read CSV\n",
    "train_df = pd.read_csv(TRAIN_PATH_2)\n",
    "valid_df = pd.read_csv(VALID_PATH_2)\n",
    "test_df = pd.read_csv(TEST_PATH_2)\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word2vec_model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# Function to convert a sentence to a Word2Vec vector\n",
    "def sentence_to_word2vec(sentence, model):\n",
    "    words = sentence.split()\n",
    "    word_vectors = [model[word] for word in words if word in model]\n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(300)  # Assuming Word2Vec vectors are 300-dimensional\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Convert text data to Word2Vec vectors\n",
    "X_train_word2vec = np.array([sentence_to_word2vec(sentence, word2vec_model) for sentence in train_df.iloc[:, 0]])\n",
    "X_valid_word2vec = np.array([sentence_to_word2vec(sentence, word2vec_model) for sentence in valid_df.iloc[:, 0]])\n",
    "X_test_word2vec = np.array([sentence_to_word2vec(sentence, word2vec_model) for sentence in test_df.iloc[:, 0]])\n",
    "\n",
    "y_train = train_df.iloc[:, 1].values\n",
    "y_valid = valid_df.iloc[:, 1].values\n",
    "y_test = test_df.iloc[:, 1].values\n",
    "\n",
    "# Convert Word2Vec vectors to tensors\n",
    "X_train_word2vec = torch.FloatTensor(X_train_word2vec)\n",
    "X_valid_word2vec = torch.FloatTensor(X_valid_word2vec)\n",
    "X_test_word2vec = torch.FloatTensor(X_test_word2vec)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_valid = torch.LongTensor(y_valid)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# Construct Dataloader\n",
    "train_loader = DataLoader(TensorDataset(X_train_word2vec, y_train), batch_size=BATCH_SIZE_2, shuffle=True)\n",
    "valid_loader = DataLoader(TensorDataset(X_valid_word2vec, y_valid), batch_size=BATCH_SIZE_2)\n",
    "test_loader = DataLoader(TensorDataset(X_test_word2vec, y_test), batch_size=BATCH_SIZE_2)\n",
    "\n",
    "INPUT_DIM = X_train_word2vec.shape[1]\n",
    "\n",
    "print(\"Data loaders created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2036cfc2-b5f8-4c14-85cb-98224868806b",
   "metadata": {},
   "source": [
    "### 2.1.2 TF-IDF (Only one of 2.1.1 and 2.1.2 can be chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8001c4f-a348-4760-840e-3352fbc5e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Read CSV\n",
    "# train_df = pd.read_csv(TRAIN_PATH_2)\n",
    "# valid_df = pd.read_csv(VALID_PATH_2)\n",
    "# test_df = pd.read_csv(TEST_PATH_2)\n",
    "\n",
    "# # Get TF-IDF vectors\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train_tfidf = vectorizer.fit_transform(train_df.iloc[:, 0])\n",
    "# X_valid_tfidf = vectorizer.transform(valid_df.iloc[:, 0])\n",
    "# X_test_tfidf = vectorizer.transform(test_df.iloc[:, 0])\n",
    "\n",
    "# y_train = train_df.iloc[:, 1].values\n",
    "# y_valid = valid_df.iloc[:, 1].values\n",
    "# y_test = test_df.iloc[:, 1].values\n",
    "\n",
    "# # Convert TF-IDF vectors to tensors\n",
    "# X_train_tfidf = torch.FloatTensor(X_train_tfidf.toarray())\n",
    "# X_valid_tfidf = torch.FloatTensor(X_valid_tfidf.toarray())\n",
    "# X_test_tfidf = torch.FloatTensor(X_test_tfidf.toarray())\n",
    "# y_train = torch.LongTensor(y_train)\n",
    "# y_valid = torch.LongTensor(y_valid)\n",
    "# y_test = torch.LongTensor(y_test)\n",
    "\n",
    "# # Construct Dataloader\n",
    "# train_loader = DataLoader(TensorDataset(X_train_tfidf, y_train), batch_size=BATCH_SIZE_2, shuffle=True)\n",
    "# valid_loader = DataLoader(TensorDataset(X_valid_tfidf, y_valid), batch_size=BATCH_SIZE_2)\n",
    "# test_loader = DataLoader(TensorDataset(X_test_tfidf, y_test), batch_size=BATCH_SIZE_2)\n",
    "\n",
    "# INPUT_DIM = X_train_tfidf.shape[1]\n",
    "\n",
    "# print(\"Data loaders created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8abb0ad-7dc9-4650-85e6-6b715ac56482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Class\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(hidden_dims)):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(input_dim, hidden_dims[i]))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_dims[i-1], hidden_dims[i]))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        # Add the final layer\n",
    "        layers.append(nn.Linear(hidden_dims[-1], output_dim))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb8f51e-5c3f-4f65-a57c-4bdb6a3427d0",
   "metadata": {},
   "source": [
    "## 2.2 Define Distribution Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d789a48-da47-415f-8a0c-796af8120416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes=31, kernel_sizes=[3, 4, 5], num_filters=10, sum_distribution=256):\n",
    "        super(GeneratorNetwork, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (K, embed_dim)) for K in kernel_sizes\n",
    "        ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "        self.sum_distribution = sum_distribution\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3)  # (batch_size, num_filters, ~)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()  # Ensure input is of type LongTensor\n",
    "        x = self.embedding(x).unsqueeze(1)  # (batch_size, 1, seq_len, embed_dim)\n",
    "        x = torch.cat([self.conv_and_pool(x, conv) for conv in self.convs], 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "vocab_size = 50  # Example vocabulary size\n",
    "embed_dim = 256  # Example embedding dimension\n",
    "# model = GeneratorNetwork(vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e17581-dfad-40d3-8e14-9df3413fb7d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.3 Define the Integratede Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f719ff73-d0ab-4cf9-9ff3-dcdf3b1cc086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator + Classifier\n",
    "class IntegratedClassifier(nn.Module):\n",
    "    def __init__(self, distribution_generator, distribution_classifier):\n",
    "        super(IntegratedClassifier, self).__init__()\n",
    "        self.distribution_generator = distribution_generator\n",
    "        self.distribution_classifier = distribution_classifier\n",
    "    \n",
    "    def forward(self, x):\n",
    "        distribution = self.distribution_generator(x)\n",
    "        return self.distribution_classifier(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3311f786-f8e7-4af8-97a6-fd62e6e537dc",
   "metadata": {},
   "source": [
    "## 2.4 Define Train and Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "486381fd-937a-4596-b74c-c25c8e027070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, criterion, optimizer, train_loader, valid_loader, num_epochs=EPOCH_2):\n",
    "    best_valid_loss = float('inf')\n",
    "    best_model = None\n",
    "    train_losses, valid_losses = [], []\n",
    "    train_accuracies, valid_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_train_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += y_batch.size(0)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "        \n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        epoch_valid_loss = 0\n",
    "        correct_valid = 0\n",
    "        total_valid = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in valid_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                epoch_valid_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_valid += y_batch.size(0)\n",
    "                correct_valid += (predicted == y_batch).sum().item()\n",
    "            \n",
    "            valid_losses.append(epoch_valid_loss / len(valid_loader))\n",
    "            valid_accuracy = correct_valid / total_valid\n",
    "            valid_accuracies.append(valid_accuracies)\n",
    "            \n",
    "            # Save Best Model\n",
    "            if epoch_valid_loss < best_valid_loss:\n",
    "                best_valid_loss = epoch_valid_loss\n",
    "                best_model = model.state_dict()\n",
    "        \n",
    "        # The loss and accuracy for each epoch are output\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}:')\n",
    "            print(f'Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "            print(f'Valid Loss: {valid_losses[-1]:.4f}, Valid Accuracy: {valid_accuracy:.4f}')\n",
    "    \n",
    "    return best_model, train_losses, valid_losses, train_accuracies, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b571e4-b155-4059-b059-93aef3b046fb",
   "metadata": {},
   "source": [
    "## 2.5 Train and Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfe203b1-f085-436e-b308-220a2512d8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50:\n",
      "Train Loss: 1.6102, Train Accuracy: 0.2011\n",
      "Valid Loss: 1.6100, Valid Accuracy: 0.2000\n",
      "Epoch 20/50:\n",
      "Train Loss: 1.6101, Train Accuracy: 0.2011\n",
      "Valid Loss: 1.6111, Valid Accuracy: 0.2000\n",
      "Epoch 30/50:\n",
      "Train Loss: 1.6098, Train Accuracy: 0.2011\n",
      "Valid Loss: 1.6110, Valid Accuracy: 0.2000\n",
      "Epoch 40/50:\n",
      "Train Loss: 1.6095, Train Accuracy: 0.2011\n",
      "Valid Loss: 1.6108, Valid Accuracy: 0.2000\n",
      "Epoch 50/50:\n",
      "Train Loss: 1.6096, Train Accuracy: 0.2011\n",
      "Valid Loss: 1.6111, Valid Accuracy: 0.2000\n",
      "The model has been saved to ../model/best_generator.pth\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "distribution_generator = GeneratorNetwork(vocab_size, embed_dim)\n",
    "# distribution_generator = GeneratorNetwork(INPUT_DIM, HIDDEN_DIM_2)\n",
    "distribution_classifier = MLPClassifier(INTER_DIM, HIDDEN_DIM_1, LABEL_NUM)\n",
    "distribution_classifier.load_state_dict(torch.load(SAVE_MODEL_PATH_1))\n",
    "integrated_classifier = IntegratedClassifier(distribution_generator, distribution_classifier)\n",
    "integrated_classifier.distribution_classifier.requires_grad_(False)\n",
    "\n",
    "# Optimization\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(integrated_classifier.parameters(), lr=LEARNING_RATE_2)\n",
    "\n",
    "# Train the model\n",
    "best_model, train_losses, valid_losses, train_accuracies, valid_accuracies = train_and_evaluate(integrated_classifier, criterion, optimizer, train_loader, valid_loader)\n",
    "\n",
    "# Save the best model\n",
    "save_path = SAVE_MODEL_PATH_2\n",
    "torch.save(best_model, save_path)\n",
    "print(f\"The model has been saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da9e081-f216-4e52-a57a-49bf92dbd467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training Process Visualization\n",
    "# plt.figure(figsize=(12, 5))\n",
    "\n",
    "# plt.plot(train_losses, label='Train Loss')\n",
    "# plt.plot(valid_losses, label='Valid Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Loss per Epoch')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# plt.plot(train_accuracies, label='Train Accuracy')\n",
    "# plt.plot(valid_accuracies, label='Valid Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Accuracy per Epoch')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd78e23-af94-463e-a663-2767b53551bd",
   "metadata": {},
   "source": [
    "## 2.6 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "928c990e-5b9f-4d48-942f-a3c4f0a555c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        10\n",
      "           1       0.00      0.00      0.00        10\n",
      "           2       0.20      1.00      0.33        10\n",
      "           3       0.00      0.00      0.00        10\n",
      "           4       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.20        50\n",
      "   macro avg       0.04      0.20      0.07        50\n",
      "weighted avg       0.04      0.20      0.07        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Liu\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin Liu\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Kevin Liu\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "integrated_classifier.load_state_dict(best_model)\n",
    "\n",
    "integrated_classifier.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = integrated_classifier(X_batch)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_test, all_preds)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(classification_report(y_test, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a6702ff-9ce6-411c-80a8-5c9e39f2eae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_correct = [0] * 5\n",
    "# class_total = [0] * 5\n",
    "# for i in range(len(y_test)):\n",
    "#     label = y_test[i]\n",
    "#     class_correct[label] += (all_preds[i] == label)\n",
    "#     class_total[label] += 1\n",
    "\n",
    "# class_accuracy = [c / t for c, t in zip(class_correct, class_total)]\n",
    "\n",
    "# plt.bar(range(5), class_accuracy, color='blue', alpha=0.6, label='Correct')\n",
    "# plt.xlabel('Class')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Per-Class Accuracy')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff00719-1fb2-403e-8c66-306ec8d13f58",
   "metadata": {},
   "source": [
    "## 2.7 Generate Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfff30ad-7867-4c20-b25e-1dc6df329505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `test_loader` is already defined and loaded with test data\n",
    "def get_generator_output(test_loader, generator_model, sum_distribution=256):\n",
    "    generator_model.eval()\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_batch, _ in test_loader:\n",
    "            x = generator_model(x_batch)\n",
    "            x = torch.softmax(x, dim=1) * sum_distribution\n",
    "            x = torch.round(x)\n",
    "            \n",
    "            sum_x = x.sum(dim=1, keepdim=True)\n",
    "            diff = sum_distribution - sum_x\n",
    "\n",
    "            for i in range(x.size(0)):\n",
    "                while diff[i] != 0:\n",
    "                    if diff[i] > 0:\n",
    "                        idx = torch.argmin(x[i])\n",
    "                        x[i][idx] += 1\n",
    "                        diff[i] -= 1\n",
    "                    else:\n",
    "                        idx = torch.argmax(x[i])\n",
    "                        x[i][idx] -= 1\n",
    "                        diff[i] += 1\n",
    "            \n",
    "            all_outputs.append(x)\n",
    "    \n",
    "    return torch.cat(all_outputs, dim=0)\n",
    "\n",
    "def save_output_to_csv_tensor(output_tensor, filename):\n",
    "    output_df = pd.DataFrame(output_tensor.cpu().numpy())\n",
    "    output_df.to_csv(filename, index=False)\n",
    "    print(f\"Output saved to {filename}\")\n",
    "    \n",
    "def save_output_to_csv_else(output, filename):\n",
    "    output_df = pd.DataFrame(output)\n",
    "    output_df.to_csv(filename, index=False)\n",
    "    print(f\"Output saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac442cb3-491e-497b-8927-ee8beaad21c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to ../test_distribution.csv\n",
      "Output saved to ../test_prediction.csv\n",
      "Output saved to ../test_true_label.csv\n"
     ]
    }
   ],
   "source": [
    "# Assuming `tfidf_network` and `distribution_classifier` are already defined and loaded with the trained models\n",
    "test_output = get_generator_output(test_loader, integrated_classifier.distribution_generator)\n",
    "\n",
    "# Save the output to CSV\n",
    "save_output_to_csv_tensor(test_output, OUTPUT_PATH)\n",
    "save_output_to_csv_else(all_preds, PREDICTION_PATH)\n",
    "save_output_to_csv_else(y_test, TRUE_LABEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f7cb3f-9bc0-45f6-bbec-207bb4af4b63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myConda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
